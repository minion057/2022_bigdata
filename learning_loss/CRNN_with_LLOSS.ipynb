{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import glob \n",
    "import pandas as pd\n",
    "import string\n",
    "import collections\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import models.lossnet as lossnet\n",
    "import models.crnn as crnn\n",
    "from config import *\n",
    "from sampler import SubsetSequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = glob.glob(os.path.join('./Large_Captcha_Dataset', '*.png'))\n",
    "path = './Large_Captcha_Dataset'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data.remove(os.path.join('./Large_Captcha_Dataset', '4q2wA.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82328"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_letters = string.ascii_uppercase + string.ascii_lowercase + string.digits\n",
    "\n",
    "mapping = {}\n",
    "mapping_inv = {}\n",
    "i = 1\n",
    "for x in all_letters:\n",
    "    mapping[x] = i\n",
    "    mapping_inv[i] = x\n",
    "    i += 1\n",
    "\n",
    "num_class = len(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d277h.png</td>\n",
       "      <td>[30, 55, 60, 60, 34]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>V8ITg.png</td>\n",
       "      <td>[22, 61, 9, 20, 33]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YO1J7.png</td>\n",
       "      <td>[25, 15, 54, 10, 60]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5n9x1.png</td>\n",
       "      <td>[58, 40, 62, 50, 54]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9E0m0.png</td>\n",
       "      <td>[62, 5, 53, 39, 53]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image                 label\n",
       "0  d277h.png  [30, 55, 60, 60, 34]\n",
       "1  V8ITg.png   [22, 61, 9, 20, 33]\n",
       "2  YO1J7.png  [25, 15, 54, 10, 60]\n",
       "3  5n9x1.png  [58, 40, 62, 50, 54]\n",
       "4  9E0m0.png   [62, 5, 53, 39, 53]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = []\n",
    "labels = []\n",
    "datas = collections.defaultdict(list)\n",
    "for d in data:\n",
    "    x = d.split('/')[-1]\n",
    "    datas['image'].append(x)\n",
    "    datas['label'].append([mapping[i] for i in x.split('.')[0]])\n",
    "df = pd.DataFrame(datas)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptchaDataset:\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.df.iloc[idx]\n",
    "        image = Image.open(os.path.join(path, data['image'])).convert('L')\n",
    "        label = torch.tensor(data['label'], dtype=torch.int32)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2, shuffle=True)\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    # T.RandomHorizontalFlip(),\n",
    "    # T.RandomCrop(size=32, padding=4),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.4914,], [0.2023,]) # T.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)) # CIFAR-100\n",
    "])\n",
    "\n",
    "test_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.4914,], [0.2023,]) # T.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)) # CIFAR-100\n",
    "])\n",
    "\n",
    "trainset = CaptchaDataset(df_train, train_transform)\n",
    "unlabeledset = CaptchaDataset(df_train, test_transform)\n",
    "testset = CaptchaDataset(df_test, test_transform)\n",
    "\n",
    "# trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "# testloader = DataLoader(testset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Prediction Loss\n",
    "def LossPredLoss(input, target, margin=1.0, reduction='mean'):\n",
    "    assert len(input) % 2 == 0, 'the batch size is not even.'\n",
    "    assert input.shape == input.flip(0).shape\n",
    "    \n",
    "    input = (input - input.flip(0))[:len(input)//2] # [l_1 - l_2B, l_2 - l_2B-1, ... , l_B - l_B+1], where batch_size = 2B\n",
    "    target = (target - target.flip(0))[:len(target)//2]\n",
    "    target = target.detach()\n",
    "\n",
    "    one = 2 * torch.sign(torch.clamp(target, min=0)) - 1 # 1 operation which is defined by the authors\n",
    "    \n",
    "    if reduction == 'mean':\n",
    "        loss = torch.sum(torch.clamp(margin - one * input, min=0))\n",
    "        loss = loss / input.size(0) # Note that the size of input is already halved\n",
    "    elif reduction == 'none':\n",
    "        loss = torch.clamp(margin - one * input, min=0)\n",
    "    else:\n",
    "        NotImplementedError()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uncertainty(models, criterion, unlabeled_loader):\n",
    "    models['backbone'].eval()\n",
    "    models['module'].eval()\n",
    "    uncertainty = torch.tensor([]).cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (inputs, labels) in unlabeled_loader:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            scores, features, _ = models['backbone'](inputs, labels, criterion)\n",
    "            pred_loss = models['module'](features) # pred_loss = criterion(scores, labels) # ground truth loss\n",
    "            pred_loss = pred_loss.view(pred_loss.size(0))\n",
    "\n",
    "            uncertainty = torch.cat((uncertainty, pred_loss), 0)\n",
    "    \n",
    "    return uncertainty.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(outputs):\n",
    "    result = []\n",
    "    for i in range(len(outputs)):\n",
    "        pred = []\n",
    "        then = 0\n",
    "        for x in outputs[i]:\n",
    "            if then != x and x > 0 :\n",
    "                pred.append(x)\n",
    "                if len(pred) == 5:\n",
    "                    break\n",
    "            then = x\n",
    "        if len(pred) < 5:\n",
    "            for i in range(5-len(pred)):\n",
    "                pred.append(0)\n",
    "        result.append(pred)\n",
    "    result = torch.LongTensor(result).cuda()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Utils\n",
    "iters = 0\n",
    "\n",
    "#\n",
    "def train_epoch(models, criterion, optimizers, dataloaders, epoch, epoch_loss, vis=None, plot_data=None):\n",
    "    models['backbone'].train()\n",
    "    models['module'].train()\n",
    "    global iters\n",
    "\n",
    "    # loss_cum_1, loss_cum_2 = 0, 0\n",
    "\n",
    "    for data in tqdm(dataloaders['train'], leave=False, total=len(dataloaders['train'])):\n",
    "        inputs = data[0].cuda()\n",
    "        labels = data[1].cuda()\n",
    "        iters += 1\n",
    "\n",
    "        optimizers['backbone'].zero_grad()\n",
    "        optimizers['module'].zero_grad()\n",
    "\n",
    "        scores, features, target_loss = models['backbone'](inputs, labels, criterion)\n",
    "        # target_loss = target_loss.log_softmax(-1)\n",
    "        # target_loss = criterion(scores, labels)\n",
    "\n",
    "        if epoch > epoch_loss:\n",
    "            # After 120 epochs, stop the gradient from the loss prediction module propagated to the target model.\n",
    "            features[0] = features[0].detach()\n",
    "            features[1] = features[1].detach()\n",
    "            features[2] = features[2].detach()\n",
    "            # features[3] = features[3].detach()\n",
    "        pred_loss = models['module'](features)\n",
    "        pred_loss = pred_loss.view(pred_loss.size(0))\n",
    "\n",
    "        m_backbone_loss = torch.sum(target_loss) / target_loss.size(0)\n",
    "        m_module_loss   = LossPredLoss(pred_loss, target_loss, margin=MARGIN)\n",
    "        loss            = m_backbone_loss + WEIGHT * m_module_loss\n",
    "        # loss_cum_1 += m_backbone_loss\n",
    "        # loss_cum_2 += m_module_loss\n",
    "        # print(f'm_backbone_loss : {m_backbone_loss}, m_module_loss : {m_module_loss}, loss : {loss}')\n",
    "\n",
    "        loss.backward()\n",
    "        optimizers['backbone'].step()\n",
    "        optimizers['module'].step()\n",
    "\n",
    "    # return loss_cum_1, loss_cum_2\n",
    "        # Visualize\n",
    "        # if (iters % 100 == 0) and (vis != None) and (plot_data != None):\n",
    "        #     plot_data['X'].append(iters)\n",
    "        #     plot_data['Y'].append([\n",
    "        #         m_backbone_loss.item(),\n",
    "        #         m_module_loss.item(),\n",
    "        #         loss.item()\n",
    "        #     ])\n",
    "        #     vis.line(\n",
    "        #         X=np.stack([np.array(plot_data['X'])] * len(plot_data['legend']), 1),\n",
    "        #         Y=np.array(plot_data['Y']),\n",
    "        #         opts={\n",
    "        #             'title': 'Loss over Time',\n",
    "        #             'legend': plot_data['legend'],\n",
    "        #             'xlabel': 'Iterations',\n",
    "        #             'ylabel': 'Loss',\n",
    "        #             'width': 1200,\n",
    "        #             'height': 390,\n",
    "        #         },\n",
    "        #         win=1\n",
    "        #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(models, criterion, dataloaders, mode='val'):\n",
    "    assert mode == 'val' or mode == 'test'\n",
    "    models['backbone'].eval()\n",
    "    models['module'].eval()\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (inputs, labels) in enumerate(dataloaders[mode]):\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "            # print(idx)\n",
    "            # print(f'!!!!!!!!!!!! input size : {inputs.size()} label size : {labels.size()}')\n",
    "            scores, _, _ = models['backbone'](inputs, labels, criterion)\n",
    "            scores = scores.permute(1, 0, 2)\n",
    "            _, preds = torch.max(scores.data, 2)\n",
    "            # print(f'!!!!!!!!!!!! previous pred : {preds}')\n",
    "            preds = predict(preds)\n",
    "            # print(f'!!!!!!!!!!!! pred size : {preds.size()}, labels : {labels.size()}, pred : {preds}, labels : {labels}')\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "            if idx == 100:\n",
    "                break\n",
    "    \n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(models, criterion, optimizers, schedulers, dataloaders, num_epochs, epoch_loss, vis, plot_data):\n",
    "    print('>> Train a Model.')\n",
    "    best_acc = 0.\n",
    "    checkpoint_dir = os.path.join('./checkpoints', 'train', 'weights')\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # schedulers['backbone'].step()\n",
    "        # schedulers['module'].step()\n",
    "\n",
    "        train_epoch(models, criterion, optimizers, dataloaders, epoch, epoch_loss, vis, plot_data)\n",
    "        # print(f'loss_cum_1 : {loss_cum_1}, loss_cum_2 : {loss_cum_2}')\n",
    "\n",
    "        # Save a checkpoint\n",
    "        # if True and epoch % 4 == 0:\n",
    "            # acc = test(models, criterion, dataloaders, 'test')\n",
    "            # if best_acc < acc:\n",
    "            #     best_acc = acc\n",
    "            #     torch.save({\n",
    "            #         'epoch': epoch + 1,\n",
    "            #         'state_dict_backbone': models['backbone'].state_dict(),\n",
    "            #         'state_dict_module': models['module'].state_dict()\n",
    "            #     },\n",
    "            #     '%s/crnn_lloss.pth' % (checkpoint_dir))\n",
    "            # print('Val Acc: {:.3f} \\t Best Acc: {:.3f}'.format(acc, best_acc))\n",
    "    print('>> Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Train a Model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_LLOSS.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_LLOSS.ipynb#ch0000021vscode-remote?line=36'>37</a>\u001b[0m schedulers \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_LLOSS.ipynb#ch0000021vscode-remote?line=37'>38</a>\u001b[0m \u001b[39m# schedulers = {'backbone': sched_backbone, 'module': sched_module}\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_LLOSS.ipynb#ch0000021vscode-remote?line=38'>39</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_LLOSS.ipynb#ch0000021vscode-remote?line=39'>40</a>\u001b[0m \u001b[39m# Training and test\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_LLOSS.ipynb#ch0000021vscode-remote?line=40'>41</a>\u001b[0m train(models, criterion, optimizers, schedulers, dataloaders, EPOCH, EPOCHL, vis, plot_data)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_LLOSS.ipynb#ch0000021vscode-remote?line=41'>42</a>\u001b[0m acc \u001b[39m=\u001b[39m test(models, criterion, dataloaders, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_LLOSS.ipynb#ch0000021vscode-remote?line=42'>43</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTrial \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m || Cycle \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m || Label set size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m: Test acc \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(trial\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, TRIALS, cycle\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, CYCLES, \u001b[39mlen\u001b[39m(labeled_set), acc))\n",
      "\u001b[1;32m/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_LLOSS.ipynb Cell 12'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(models, criterion, optimizers, schedulers, dataloaders, num_epochs, epoch_loss, vis, plot_data)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_LLOSS.ipynb#ch0000019vscode-remote?line=5'>6</a>\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(checkpoint_dir)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_LLOSS.ipynb#ch0000019vscode-remote?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_LLOSS.ipynb#ch0000019vscode-remote?line=8'>9</a>\u001b[0m     \u001b[39m# schedulers['backbone'].step()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_LLOSS.ipynb#ch0000019vscode-remote?line=9'>10</a>\u001b[0m     \u001b[39m# schedulers['module'].step()\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_LLOSS.ipynb#ch0000019vscode-remote?line=11'>12</a>\u001b[0m     train_epoch(models, criterion, optimizers, dataloaders, epoch, epoch_loss, vis, plot_data)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_LLOSS.ipynb#ch0000019vscode-remote?line=12'>13</a>\u001b[0m     \u001b[39m# print(f'loss_cum_1 : {loss_cum_1}, loss_cum_2 : {loss_cum_2}')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_LLOSS.ipynb#ch0000019vscode-remote?line=13'>14</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_LLOSS.ipynb#ch0000019vscode-remote?line=14'>15</a>\u001b[0m     \u001b[39m# Save a checkpoint\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_LLOSS.ipynb#ch0000019vscode-remote?line=24'>25</a>\u001b[0m         \u001b[39m#     '%s/crnn_lloss.pth' % (checkpoint_dir))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_LLOSS.ipynb#ch0000019vscode-remote?line=25'>26</a>\u001b[0m         \u001b[39m# print('Val Acc: {:.3f} \\t Best Acc: {:.3f}'.format(acc, best_acc))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_LLOSS.ipynb#ch0000019vscode-remote?line=26'>27</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m>> Finished.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_LLOSS.ipynb Cell 10'\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(models, criterion, optimizers, dataloaders, epoch, epoch_loss, vis, plot_data)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_LLOSS.ipynb#ch0000017vscode-remote?line=16'>17</a>\u001b[0m optimizers[\u001b[39m'\u001b[39m\u001b[39mbackbone\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_LLOSS.ipynb#ch0000017vscode-remote?line=17'>18</a>\u001b[0m optimizers[\u001b[39m'\u001b[39m\u001b[39mmodule\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_LLOSS.ipynb#ch0000017vscode-remote?line=19'>20</a>\u001b[0m scores, features, target_loss \u001b[39m=\u001b[39m models[\u001b[39m'\u001b[39;49m\u001b[39mbackbone\u001b[39;49m\u001b[39m'\u001b[39;49m](inputs, labels, criterion)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_LLOSS.ipynb#ch0000017vscode-remote?line=20'>21</a>\u001b[0m \u001b[39m# target_loss = target_loss.log_softmax(-1)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_LLOSS.ipynb#ch0000017vscode-remote?line=21'>22</a>\u001b[0m \u001b[39m# target_loss = criterion(scores, labels)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_LLOSS.ipynb#ch0000017vscode-remote?line=23'>24</a>\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m>\u001b[39m epoch_loss:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_LLOSS.ipynb#ch0000017vscode-remote?line=24'>25</a>\u001b[0m     \u001b[39m# After 120 epochs, stop the gradient from the loss prediction module propagated to the target model.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/devleop/TEMP/2022_bigdata/learning_loss/models/crnn.py:59\u001b[0m, in \u001b[0;36mCRNN.forward\u001b[0;34m(self, X, y, criterion)\u001b[0m\n\u001b[1;32m     <a href='file:///home/jyji/devleop/TEMP/2022_bigdata/learning_loss/models/crnn.py?line=55'>56</a>\u001b[0m     target_lengths \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfull(size\u001b[39m=\u001b[39m(N,), fill_value\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint32)\n\u001b[1;32m     <a href='file:///home/jyji/devleop/TEMP/2022_bigdata/learning_loss/models/crnn.py?line=57'>58</a>\u001b[0m     \u001b[39m# out3 = out3.log_softmax(-1)\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/jyji/devleop/TEMP/2022_bigdata/learning_loss/models/crnn.py?line=58'>59</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(out3, y, input_lengths, target_lengths)\n\u001b[1;32m     <a href='file:///home/jyji/devleop/TEMP/2022_bigdata/learning_loss/models/crnn.py?line=60'>61</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m out3, [out, out2, out3], loss\n\u001b[1;32m     <a href='file:///home/jyji/devleop/TEMP/2022_bigdata/learning_loss/models/crnn.py?line=62'>63</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out3, [out, out2, out3], \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/loss.py:1743\u001b[0m, in \u001b[0;36mCTCLoss.forward\u001b[0;34m(self, log_probs, targets, input_lengths, target_lengths)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/loss.py?line=1741'>1742</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, log_probs: Tensor, targets: Tensor, input_lengths: Tensor, target_lengths: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/loss.py?line=1742'>1743</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mctc_loss(log_probs, targets, input_lengths, target_lengths, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblank, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/loss.py?line=1743'>1744</a>\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mzero_infinity)\n",
      "File \u001b[0;32m~/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/functional.py:2599\u001b[0m, in \u001b[0;36mctc_loss\u001b[0;34m(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/functional.py?line=2591'>2592</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(log_probs, targets, input_lengths, target_lengths):\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/functional.py?line=2592'>2593</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/functional.py?line=2593'>2594</a>\u001b[0m         ctc_loss,\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/functional.py?line=2594'>2595</a>\u001b[0m         (log_probs, targets, input_lengths, target_lengths),\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/functional.py?line=2595'>2596</a>\u001b[0m         log_probs, targets, input_lengths, target_lengths,\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/functional.py?line=2596'>2597</a>\u001b[0m         blank\u001b[39m=\u001b[39mblank, reduction\u001b[39m=\u001b[39mreduction, zero_infinity\u001b[39m=\u001b[39mzero_infinity\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/functional.py?line=2597'>2598</a>\u001b[0m     )\n\u001b[0;32m-> <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/functional.py?line=2598'>2599</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mctc_loss(\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/functional.py?line=2599'>2600</a>\u001b[0m     log_probs, targets, input_lengths, target_lengths, blank, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), zero_infinity\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/functional.py?line=2600'>2601</a>\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vis = None\n",
    "plot_data = None\n",
    "\n",
    "for trial in range(TRIALS):\n",
    "    # Initialize a labeled dataset by randomly sampling K=ADDENDUM=1,000 data points from the entire dataset.\n",
    "    indices = list(range(NUM_TRAIN))\n",
    "    random.shuffle(indices)\n",
    "    labeled_set = indices[:ADDENDUM]\n",
    "    unlabeled_set = indices[ADDENDUM:]\n",
    "    \n",
    "    train_loader = DataLoader(trainset, batch_size=BATCH, \n",
    "                                sampler=SubsetRandomSampler(labeled_set), \n",
    "                                pin_memory=True)\n",
    "    test_loader  = DataLoader(testset, batch_size=BATCH)\n",
    "    dataloaders  = {'train': train_loader, 'test': test_loader}\n",
    "    \n",
    "    # Model\n",
    "    crnn_model    = crnn.CRNN(in_channels=1, output=num_class).cuda()\n",
    "    loss_module = lossnet.LossNet().cuda()\n",
    "    models      = {'backbone': crnn_model, 'module': loss_module}\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Active learning cycles\n",
    "    for cycle in range(CYCLES):\n",
    "        # Loss, criterion and scheduler (re)initialization\n",
    "        criterion      = nn.CTCLoss(reduction='none') # nn.CrossEntropyLoss(reduction='none')\n",
    "        optim_backbone = optim.Adam(models['backbone'].parameters(), lr=LR,\n",
    "                                weight_decay=WDECAY)\n",
    "        # optim_backbone = optim.SGD(models['backbone'].parameters(), lr=LR, \n",
    "        #                         momentum=MOMENTUM, weight_decay=WDECAY)\n",
    "        optim_module   = optim.SGD(models['module'].parameters(), lr=LR, \n",
    "                                momentum=MOMENTUM, weight_decay=WDECAY)\n",
    "        # sched_backbone = lr_scheduler.MultiStepLR(optim_backbone, milestones=MILESTONES)\n",
    "        # sched_module   = lr_scheduler.MultiStepLR(optim_module, milestones=MILESTONES)\n",
    "\n",
    "        optimizers = {'backbone': optim_backbone, 'module': optim_module}\n",
    "        schedulers = None\n",
    "        # schedulers = {'backbone': sched_backbone, 'module': sched_module}\n",
    "\n",
    "        # Training and test\n",
    "        train(models, criterion, optimizers, schedulers, dataloaders, EPOCH, EPOCHL, vis, plot_data)\n",
    "        acc = test(models, criterion, dataloaders, mode='test')\n",
    "        print('Trial {}/{} || Cycle {}/{} || Label set size {}: Test acc {}'.format(trial+1, TRIALS, cycle+1, CYCLES, len(labeled_set), acc))\n",
    "\n",
    "        ##\n",
    "        #  Update the labeled dataset via loss prediction-based uncertainty measurement\n",
    "\n",
    "        # Randomly sample 10000 unlabeled data points\n",
    "        random.shuffle(unlabeled_set)\n",
    "        subset = unlabeled_set[:SUBSET]\n",
    "\n",
    "        # Create unlabeled dataloader for the unlabeled subset\n",
    "        unlabeled_loader = DataLoader(unlabeledset, batch_size=BATCH, \n",
    "                                        sampler=SubsetSequentialSampler(subset), # more convenient if we maintain the order of subset\n",
    "                                        pin_memory=True)\n",
    "\n",
    "        # Measure uncertainty of each data points in the subset\n",
    "        uncertainty = get_uncertainty(models, criterion, unlabeled_loader)\n",
    "\n",
    "        # Index in ascending order\n",
    "        arg = np.argsort(uncertainty)\n",
    "        \n",
    "        # Update the labeled dataset and the unlabeled dataset, respectively\n",
    "        labeled_set += list(torch.tensor(subset)[arg][-ADDENDUM:].numpy())\n",
    "        unlabeled_set = list(torch.tensor(subset)[arg][:-ADDENDUM].numpy()) + unlabeled_set[SUBSET:]\n",
    "\n",
    "        # Create a new dataloader for the updated labeled dataset\n",
    "        dataloaders['train'] = DataLoader(trainset, batch_size=BATCH, \n",
    "                                            sampler=SubsetRandomSampler(labeled_set), \n",
    "                                            pin_memory=True)\n",
    "    \n",
    "    # Save a checkpoint\n",
    "    torch.save({\n",
    "                'trial': trial + 1,\n",
    "                'state_dict_backbone': models['backbone'].state_dict(),\n",
    "                'state_dict_module': models['module'].state_dict()\n",
    "            },\n",
    "            './checkpoints/trial/weights/crnn_lloss_{}.pth'.format(trial))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b4a27c98caeaa5630be10b8d406a4608184d11e4add7ee29d27ce8c7f4d0bc9a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('mytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
