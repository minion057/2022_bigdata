{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import glob \n",
    "import pandas as pd\n",
    "import string\n",
    "import collections\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import models.lossnet as lossnet\n",
    "import models.crnn as crnn\n",
    "from config import *\n",
    "from sampler import SubsetSequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = glob.glob(os.path.join('./Large_Captcha_Dataset', '*.png'))\n",
    "path = './Large_Captcha_Dataset'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data.remove(os.path.join('./Large_Captcha_Dataset', '4q2wA.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_letters = string.ascii_uppercase + string.ascii_lowercase + string.digits\n",
    "\n",
    "mapping = {}\n",
    "mapping_inv = {}\n",
    "i = 1\n",
    "for x in all_letters:\n",
    "    mapping[x] = i\n",
    "    mapping_inv[i] = x\n",
    "    i += 1\n",
    "\n",
    "num_class = len(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = []\n",
    "datas = collections.defaultdict(list)\n",
    "for d in data:\n",
    "    x = d.split('/')[-1]\n",
    "    datas['image'].append(x)\n",
    "    datas['label'].append([mapping[i] for i in x.split('.')[0]])\n",
    "df = pd.DataFrame(datas)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptchaDataset:\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.df.iloc[idx]\n",
    "        image = Image.open(os.path.join(path, data['image'])).convert('L')\n",
    "        label = torch.tensor(data['label'], dtype=torch.int32)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2, shuffle=True)\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.4914,], [0.2023,])\n",
    "])\n",
    "\n",
    "test_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.4914,], [0.2023,])\n",
    "])\n",
    "\n",
    "trainset = CaptchaDataset(df_train, train_transform)\n",
    "unlabeledset = CaptchaDataset(df_train, test_transform)\n",
    "testset = CaptchaDataset(df_test, test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Prediction Loss\n",
    "def LossPredLoss(input, target, margin=1.0, reduction='mean'):\n",
    "    assert len(input) % 2 == 0, 'the batch size is not even.'\n",
    "    assert input.shape == input.flip(0).shape\n",
    "    \n",
    "    input = (input - input.flip(0))[:len(input)//2] # [l_1 - l_2B, l_2 - l_2B-1, ... , l_B - l_B+1], where batch_size = 2B\n",
    "    target = (target - target.flip(0))[:len(target)//2]\n",
    "    target = target.detach()\n",
    "\n",
    "    one = 2 * torch.sign(torch.clamp(target, min=0)) - 1 # 1 operation which is defined by the authors\n",
    "    \n",
    "    if reduction == 'mean':\n",
    "        loss = torch.sum(torch.clamp(margin - one * input, min=0))\n",
    "        loss = loss / input.size(0) # Note that the size of input is already halved\n",
    "    elif reduction == 'none':\n",
    "        loss = torch.clamp(margin - one * input, min=0)\n",
    "    else:\n",
    "        NotImplementedError()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uncertainty(models, criterion, unlabeled_loader):\n",
    "    models['backbone'].eval()\n",
    "    models['module'].eval()\n",
    "    uncertainty = torch.tensor([]).cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (inputs, labels) in unlabeled_loader:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            scores, features, _ = models['backbone'](inputs, labels, criterion)\n",
    "            pred_loss = models['module'](features) # pred_loss = criterion(scores, labels) # ground truth loss\n",
    "            pred_loss = pred_loss.view(pred_loss.size(0))\n",
    "\n",
    "            uncertainty = torch.cat((uncertainty, pred_loss), 0)\n",
    "    \n",
    "    return uncertainty.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(outputs):\n",
    "    result = []\n",
    "    for i in range(len(outputs)):\n",
    "        pred = []\n",
    "        then = 0\n",
    "        for x in outputs[i]:\n",
    "            if then != x and x > 0 :\n",
    "                pred.append(x)\n",
    "                if len(pred) == 5:\n",
    "                    break\n",
    "            then = x\n",
    "        if len(pred) < 5:\n",
    "            for i in range(5-len(pred)):\n",
    "                pred.append(0)\n",
    "        result.append(pred)\n",
    "    result = torch.LongTensor(result).cuda()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Utils\n",
    "iters = 0\n",
    "\n",
    "#\n",
    "def train_epoch(models, criterion, optimizers, dataloaders, num_epochs, epoch, epoch_loss):\n",
    "    models['backbone'].train()\n",
    "    models['module'].train()\n",
    "    global iters\n",
    "\n",
    "    # loss_cum_1, loss_cum_2 = 0, 0\n",
    "\n",
    "    pbar = tqdm(dataloaders['train'], leave=False, total=len(dataloaders['train']))\n",
    "    for idx, data in enumerate(pbar):\n",
    "        inputs = data[0].cuda()\n",
    "        labels = data[1].cuda()\n",
    "        iters += 1\n",
    "\n",
    "        optimizers['backbone'].zero_grad()\n",
    "        optimizers['module'].zero_grad()\n",
    "\n",
    "        scores, features, target_loss = models['backbone'](inputs, labels, criterion)\n",
    "        # target_loss = target_loss.log_softmax(-1)\n",
    "        # target_loss = criterion(scores, labels)\n",
    "\n",
    "        if epoch > epoch_loss:\n",
    "            # After 120 epochs, stop the gradient from the loss prediction module propagated to the target model.\n",
    "            features[0] = features[0].detach()\n",
    "            features[1] = features[1].detach()\n",
    "            features[2] = features[2].detach()\n",
    "            # features[3] = features[3].detach()\n",
    "        pred_loss = models['module'](features)\n",
    "        pred_loss = pred_loss.view(pred_loss.size(0))\n",
    "\n",
    "        m_backbone_loss = torch.sum(target_loss) / target_loss.size(0)\n",
    "        m_module_loss   = LossPredLoss(pred_loss, target_loss, margin=MARGIN)\n",
    "        loss            = m_backbone_loss + WEIGHT * m_module_loss\n",
    "\n",
    "        pbar.set_description(f'({epoch}/{num_epochs}) b_loss : {m_backbone_loss:.3f}, m_loss : {m_module_loss:.3f}, loss : {loss:.3f}')\n",
    "        # loss_cum_1 += m_backbone_loss\n",
    "        # loss_cum_2 += m_module_loss\n",
    "        # print(f'm_backbone_loss : {m_backbone_loss}, m_module_loss : {m_module_loss}, loss : {loss}')\n",
    "\n",
    "        loss.backward()\n",
    "        optimizers['backbone'].step()\n",
    "        optimizers['module'].step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(models, criterion, optimizers, dataloaders, num_epochs, epoch_loss):\n",
    "    print('>> Train a Model.')\n",
    "    checkpoint_dir = os.path.join('./checkpoints', 'train', 'weights')\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_epoch(models, criterion, optimizers, dataloaders, num_epochs, epoch, epoch_loss)\n",
    "\n",
    "        # Save a checkpoint\n",
    "        if False and epoch % 10 == 0:\n",
    "            acc = test(models, criterion, dataloaders, mode='test')\n",
    "            print(f'acc : {acc}')\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict_backbone': models['backbone'].state_dict(),\n",
    "                'state_dict_module': models['module'].state_dict()\n",
    "            },\n",
    "            f'{checkpoint_dir}/crnn_lloss_{int(time.time())}.pth')\n",
    "    print('>> Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(models, criterion, dataloaders, mode='val'):\n",
    "    print('>> Test a Model.')\n",
    "    assert mode == 'val' or mode == 'test'\n",
    "    models['backbone'].eval()\n",
    "    models['module'].eval()\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (inputs, labels) in enumerate(dataloaders[mode]):\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "            scores, _, _ = models['backbone'](inputs, labels, criterion)\n",
    "            scores = scores.permute(1, 0, 2)\n",
    "            _, preds = torch.max(scores.data, 2)\n",
    "            preds = predict(preds)\n",
    "            total += labels.size(0)\n",
    "            for i in range(len(preds)):\n",
    "                correct += torch.equal(preds[i], labels[i])\n",
    "\n",
    "            if idx >= 999:\n",
    "                break\n",
    "    \n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_task():   \n",
    "    for trial in range(TRIALS):\n",
    "        # Initialize a labeled dataset by randomly sampling K=ADDENDUM=1,000 data points from the entire dataset.\n",
    "        indices = list(range(NUM_TRAIN))\n",
    "        random.shuffle(indices)\n",
    "        labeled_set = indices[:ADDENDUM]\n",
    "        unlabeled_set = indices[ADDENDUM:]\n",
    "        \n",
    "        train_loader = DataLoader(trainset, batch_size=BATCH, \n",
    "                                    sampler=SubsetRandomSampler(labeled_set), \n",
    "                                    pin_memory=True)\n",
    "        test_loader  = DataLoader(testset, batch_size=BATCH, shuffle=True)\n",
    "        dataloaders  = {'train': train_loader, 'test': test_loader}\n",
    "        \n",
    "        # Model\n",
    "        crnn_model    = crnn.CRNN(in_channels=1, output=num_class).cuda()\n",
    "        loss_module = lossnet.LossNet().cuda()\n",
    "        models      = {'backbone': crnn_model, 'module': loss_module}\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "        # Active learning cycles\n",
    "        for cycle in range(CYCLES):\n",
    "            # Initialize Model weights Not Loss Prediction Module weights\n",
    "            models['backbone'].reset_parameters()\n",
    "            # models['module'].reset_parameters()\n",
    "\n",
    "            # Loss, criterion and scheduler (re)initialization\n",
    "            criterion      = nn.CTCLoss(reduction='none') # nn.CrossEntropyLoss(reduction='none')\n",
    "            optim_backbone = optim.Adam(models['backbone'].parameters(), lr=LR,\n",
    "                                    weight_decay=WDECAY)\n",
    "            optim_module   = optim.SGD(models['module'].parameters(), lr=LR, \n",
    "                                    momentum=MOMENTUM, weight_decay=WDECAY)\n",
    "\n",
    "            optimizers = {'backbone': optim_backbone, 'module': optim_module}\n",
    "\n",
    "            # Training and test\n",
    "            # _epoch = EPOCH // (len(labeled_set)//2)\n",
    "            # _epochl = int((EPOCH // (len(labeled_set)//2)) * 0.8)\n",
    "            train(models, criterion, optimizers, dataloaders, EPOCH, EPOCHL)\n",
    "            acc = test(models, criterion, dataloaders, mode='test')\n",
    "            cur_time = time.strftime('%Y-%m-%d %I:%M:%S %p', time.localtime())\n",
    "            print('{} : Trial {}/{} || Cycle {}/{} || Label set size {}: Test acc {}'.format(cur_time, trial+1, TRIALS, cycle+1, CYCLES, len(labeled_set), acc))\n",
    "            with open('result.txt', 'a+') as f:\n",
    "                f.write('{} : Trial {}/{} || Cycle {}/{} || Label set size {}: Test acc {}\\n'.format(cur_time, trial+1, TRIALS, cycle+1, CYCLES, len(labeled_set), acc))\n",
    "\n",
    "\n",
    "            # Update the labeled dataset via loss prediction-based uncertainty measurement\n",
    "\n",
    "            # Randomly sample 10000 unlabeled data points\n",
    "            random.shuffle(unlabeled_set)\n",
    "            subset = unlabeled_set[:SUBSET]\n",
    "\n",
    "            # Create unlabeled dataloader for the unlabeled subset\n",
    "            unlabeled_loader = DataLoader(unlabeledset, batch_size=BATCH, \n",
    "                                            sampler=SubsetSequentialSampler(subset), # more convenient if we maintain the order of subset\n",
    "                                            pin_memory=True)\n",
    "\n",
    "            # Measure uncertainty of each data points in the subset\n",
    "            uncertainty = get_uncertainty(models, criterion, unlabeled_loader)\n",
    "\n",
    "            # Index in ascending order\n",
    "            arg = np.argsort(uncertainty)\n",
    "            \n",
    "            # Update the labeled dataset and the unlabeled dataset, respectively\n",
    "            # labeled_set += list(torch.tensor(subset)[arg][-ADDENDUM:].numpy())\n",
    "            # unlabeled_set = list(torch.tensor(subset)[arg][:-ADDENDUM].numpy()) + unlabeled_set[SUBSET:]\n",
    "            labeled_set += list(torch.tensor(subset)[arg][-int(ADDENDUM/2):].numpy())\n",
    "            labeled_set += list(torch.tensor(subset)[arg][:int(ADDENDUM/2)].numpy())\n",
    "            unlabeled_set = list(torch.tensor(subset)[arg][int(ADDENDUM/2):-int(ADDENDUM/2)].numpy()) + unlabeled_set[SUBSET:]\n",
    "\n",
    "            # Create a new dataloader for the updated labeled dataset\n",
    "            dataloaders['train'] = DataLoader(trainset, batch_size=BATCH, \n",
    "                                                sampler=SubsetRandomSampler(labeled_set), \n",
    "                                                pin_memory=True)\n",
    "        \n",
    "        checkpoint_dir = os.path.join('./checkpoints', 'trial', 'weights')\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        # Save a checkpoint\n",
    "        torch.save({\n",
    "                    'trial': trial + 1,\n",
    "                    'state_dict_backbone': models['backbone'].state_dict(),\n",
    "                    'state_dict_module': models['module'].state_dict()\n",
    "                },\n",
    "                f'{checkpoint_dir}/crnn_lloss_{int(time.time())}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Train a Model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Finished.\n",
      ">> Test a Model.\n",
      "Trial 1/1 || Cycle 1/10 || Label set size 1000: Test acc 0.0\n",
      "tensor([0.7208, 0.6658, 0.6626,  ..., 0.6834, 0.6490, 0.6441])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b4a27c98caeaa5630be10b8d406a4608184d11e4add7ee29d27ce8c7f4d0bc9a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('mytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
