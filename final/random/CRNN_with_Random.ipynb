{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import glob \n",
    "import pandas as pd\n",
    "import string\n",
    "import collections\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import models.lossnet as lossnet\n",
    "import models.crnn as crnn\n",
    "from config import *\n",
    "from sampler import SubsetSequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = glob.glob(os.path.join('./Large_Captcha_Dataset', '*.png'))\n",
    "path = './Large_Captcha_Dataset'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data.remove(os.path.join('./Large_Captcha_Dataset', '4q2wA.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_letters = string.ascii_uppercase + string.ascii_lowercase + string.digits\n",
    "\n",
    "mapping = {}\n",
    "mapping_inv = {}\n",
    "i = 1\n",
    "for x in all_letters:\n",
    "    mapping[x] = i\n",
    "    mapping_inv[i] = x\n",
    "    i += 1\n",
    "\n",
    "num_class = len(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = []\n",
    "datas = collections.defaultdict(list)\n",
    "for d in data:\n",
    "    x = d.split('/')[-1]\n",
    "    datas['image'].append(x)\n",
    "    datas['label'].append([mapping[i] for i in x.split('.')[0]])\n",
    "df = pd.DataFrame(datas)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptchaDataset:\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.df.iloc[idx]\n",
    "        image = Image.open(os.path.join(path, data['image'])).convert('L')\n",
    "        label = torch.tensor(data['label'], dtype=torch.int32)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2, shuffle=True)\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.4914,], [0.2023,])\n",
    "])\n",
    "\n",
    "test_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.4914,], [0.2023,])\n",
    "])\n",
    "\n",
    "trainset = CaptchaDataset(df_train, train_transform)\n",
    "unlabeledset = CaptchaDataset(df_train, test_transform)\n",
    "testset = CaptchaDataset(df_test, test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Prediction Loss\n",
    "def LossPredLoss(input, target, margin=1.0, reduction='mean'):\n",
    "    assert len(input) % 2 == 0, 'the batch size is not even.'\n",
    "    assert input.shape == input.flip(0).shape\n",
    "    \n",
    "    input = (input - input.flip(0))[:len(input)//2] # [l_1 - l_2B, l_2 - l_2B-1, ... , l_B - l_B+1], where batch_size = 2B\n",
    "    target = (target - target.flip(0))[:len(target)//2]\n",
    "    target = target.detach()\n",
    "\n",
    "    one = 2 * torch.sign(torch.clamp(target, min=0)) - 1 # 1 operation which is defined by the authors\n",
    "    \n",
    "    if reduction == 'mean':\n",
    "        loss = torch.sum(torch.clamp(margin - one * input, min=0))\n",
    "        loss = loss / input.size(0) # Note that the size of input is already halved\n",
    "    elif reduction == 'none':\n",
    "        loss = torch.clamp(margin - one * input, min=0)\n",
    "    else:\n",
    "        NotImplementedError()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uncertainty(models, criterion, unlabeled_loader):\n",
    "    models['backbone'].eval()\n",
    "    models['module'].eval()\n",
    "    uncertainty = torch.tensor([]).cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (inputs, labels) in unlabeled_loader:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            scores, features, _ = models['backbone'](inputs, labels, criterion)\n",
    "            pred_loss = models['module'](features) # pred_loss = criterion(scores, labels) # ground truth loss\n",
    "            pred_loss = pred_loss.view(pred_loss.size(0))\n",
    "\n",
    "            uncertainty = torch.cat((uncertainty, pred_loss), 0)\n",
    "    \n",
    "    return uncertainty.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(outputs):\n",
    "    result = []\n",
    "    for i in range(len(outputs)):\n",
    "        pred = []\n",
    "        then = 0\n",
    "        for x in outputs[i]:\n",
    "            if then != x and x > 0 :\n",
    "                pred.append(x)\n",
    "                if len(pred) == 5:\n",
    "                    break\n",
    "            then = x\n",
    "        if len(pred) < 5:\n",
    "            for i in range(5-len(pred)):\n",
    "                pred.append(0)\n",
    "        result.append(pred)\n",
    "    result = torch.LongTensor(result).cuda()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Utils\n",
    "iters = 0\n",
    "\n",
    "#\n",
    "def train_epoch(models, criterion, optimizers, dataloaders, num_epochs, epoch, epoch_loss):\n",
    "    models['backbone'].train()\n",
    "    global iters\n",
    "\n",
    "\n",
    "    pbar = tqdm(dataloaders['train'], leave=False, total=len(dataloaders['train']))\n",
    "    for idx, data in enumerate(pbar):\n",
    "        inputs = data[0].cuda()\n",
    "        labels = data[1].cuda()\n",
    "        iters += 1\n",
    "\n",
    "        optimizers['backbone'].zero_grad()\n",
    "\n",
    "        _, _, target_loss = models['backbone'](inputs, labels, criterion)\n",
    "\n",
    "        pbar.set_description(f'({epoch}/{num_epochs}) target_loss : {target_loss.item():.3f}')\n",
    "\n",
    "        target_loss.backward()\n",
    "        optimizers['backbone'].step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(models, criterion, optimizers, dataloaders, num_epochs, epoch_loss):\n",
    "    print('>> Train a Model.')\n",
    "    checkpoint_dir = os.path.join('./checkpoints', 'train', 'weights')\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_epoch(models, criterion, optimizers, dataloaders, num_epochs, epoch, epoch_loss)\n",
    "\n",
    "        # Save a checkpoint\n",
    "        if False and epoch % 10 == 0:\n",
    "            acc = test(models, criterion, dataloaders, mode='test')\n",
    "            print(f'acc : {acc}')\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict_backbone': models['backbone'].state_dict(),\n",
    "            },\n",
    "            f'{checkpoint_dir}/crnn_lloss_{int(time.time())}.pth')\n",
    "    print('>> Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(models, criterion, dataloaders, mode='val'):\n",
    "    print('>> Test a Model.')\n",
    "    assert mode == 'val' or mode == 'test'\n",
    "    models['backbone'].eval()\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (inputs, labels) in enumerate(dataloaders[mode]):\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "            scores, _, _ = models['backbone'](inputs, labels, criterion)\n",
    "            scores = scores.permute(1, 0, 2)\n",
    "            _, preds = torch.max(scores.data, 2)\n",
    "            preds = predict(preds)\n",
    "            total += labels.size(0)\n",
    "            for i in range(len(preds)):\n",
    "                correct += torch.equal(preds[i], labels[i])\n",
    "\n",
    "            if idx >= 999:\n",
    "                break\n",
    "    \n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_task():   \n",
    "    for trial in range(TRIALS):\n",
    "        # Initialize a labeled dataset by randomly sampling K=ADDENDUM=1,000 data points from the entire dataset.\n",
    "        indices = list(range(NUM_TRAIN))\n",
    "        random.shuffle(indices)\n",
    "        labeled_set = indices[:ADDENDUM]\n",
    "        unlabeled_set = indices[ADDENDUM:]\n",
    "        \n",
    "        train_loader = DataLoader(trainset, batch_size=BATCH, \n",
    "                                    sampler=SubsetRandomSampler(labeled_set), \n",
    "                                    pin_memory=True)\n",
    "        test_loader  = DataLoader(testset, batch_size=BATCH, shuffle=True)\n",
    "        dataloaders  = {'train': train_loader, 'test': test_loader}\n",
    "        \n",
    "        # Model\n",
    "        crnn_model    = crnn.CRNN(in_channels=1, output=num_class).cuda()\n",
    "        loss_module = None \n",
    "        models      = {'backbone': crnn_model, 'module': loss_module}\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "        # Active learning cycles\n",
    "        for cycle in range(CYCLES):\n",
    "            # Initialize Model weights Not Loss Prediction Module weights\n",
    "            models['backbone'].reset_parameters()\n",
    "\n",
    "            # Loss, criterion and scheduler (re)initialization\n",
    "            criterion      = nn.CTCLoss(reduction='mean') # nn.CrossEntropyLoss(reduction='none')\n",
    "            optim_backbone = optim.Adam(models['backbone'].parameters(), lr=LR,\n",
    "                                    weight_decay=WDECAY)\n",
    "            optim_module   = None \n",
    "\n",
    "            optimizers = {'backbone': optim_backbone, 'module': optim_module}\n",
    "\n",
    "            # Training and test\n",
    "            train(models, criterion, optimizers, dataloaders, EPOCH, EPOCHL)\n",
    "            acc = test(models, criterion, dataloaders, mode='test')\n",
    "            cur_time = time.strftime('%Y-%m-%d %I:%M:%S %p', time.localtime())\n",
    "            print('{} : Trial {}/{} || Cycle {}/{} || Label set size {}: Test acc {}'.format(cur_time, trial+1, TRIALS, cycle+1, CYCLES, len(labeled_set), acc))\n",
    "            with open('result.txt', 'a+') as f:\n",
    "                f.write('{} : Trial {}/{} || Cycle {}/{} || Label set size {}: Test acc {}\\n'.format(cur_time, trial+1, TRIALS, cycle+1, CYCLES, len(labeled_set), acc))\n",
    "\n",
    "\n",
    "            # Update the labeled dataset via loss prediction-based uncertainty measurement\n",
    "\n",
    "            # Randomly sample 10000 unlabeled data points\n",
    "            random.shuffle(unlabeled_set)\n",
    "            subset = unlabeled_set[:SUBSET]\n",
    "\n",
    "            # Create unlabeled dataloader for the unlabeled subset\n",
    "            unlabeled_loader = DataLoader(unlabeledset, batch_size=BATCH, \n",
    "                                            sampler=SubsetSequentialSampler(subset), # more convenient if we maintain the order of subset\n",
    "                                            pin_memory=True)\n",
    "\n",
    "\n",
    "            # Index in ascending order\n",
    "            arg = np.arange(len(subset))\n",
    "            random.shuffle(arg)\n",
    "            \n",
    "            # Update the labeled dataset and the unlabeled dataset, respectively\n",
    "            labeled_set += list(torch.tensor(subset)[arg][-ADDENDUM:].numpy())\n",
    "            unlabeled_set = list(torch.tensor(subset)[arg][:-ADDENDUM].numpy()) + unlabeled_set[SUBSET:]\n",
    "\n",
    "            # Create a new dataloader for the updated labeled dataset\n",
    "            dataloaders['train'] = DataLoader(trainset, batch_size=BATCH, \n",
    "                                                sampler=SubsetRandomSampler(labeled_set), \n",
    "                                                pin_memory=True)\n",
    "        \n",
    "        checkpoint_dir = os.path.join('./checkpoints', 'trial', 'weights')\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        # Save a checkpoint\n",
    "        torch.save({\n",
    "                    'trial': trial + 1,\n",
    "                    'state_dict_backbone': models['backbone'].state_dict(),\n",
    "                },\n",
    "                f'{checkpoint_dir}/crnn_lloss_{int(time.time())}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Train a Model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_Random.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_Random.ipynb#ch0000013vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_Random.ipynb#ch0000013vscode-remote?line=1'>2</a>\u001b[0m     main_task()\n",
      "\u001b[1;32m/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_Random.ipynb Cell 13'\u001b[0m in \u001b[0;36mmain_task\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_Random.ipynb#ch0000012vscode-remote?line=36'>37</a>\u001b[0m _epoch \u001b[39m=\u001b[39m EPOCH \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m (\u001b[39mlen\u001b[39m(labeled_set)\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_Random.ipynb#ch0000012vscode-remote?line=37'>38</a>\u001b[0m _epochl \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m((EPOCH \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m (\u001b[39mlen\u001b[39m(labeled_set)\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m)) \u001b[39m*\u001b[39m \u001b[39m0.8\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_Random.ipynb#ch0000012vscode-remote?line=38'>39</a>\u001b[0m train(models, criterion, optimizers, dataloaders, _epoch, _epochl)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_Random.ipynb#ch0000012vscode-remote?line=39'>40</a>\u001b[0m acc \u001b[39m=\u001b[39m test(models, criterion, dataloaders, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_Random.ipynb#ch0000012vscode-remote?line=40'>41</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTrial \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m || Cycle \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m || Label set size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m: Test acc \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(trial\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, TRIALS, cycle\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, CYCLES, \u001b[39mlen\u001b[39m(labeled_set), acc))\n",
      "\u001b[1;32m/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_Random.ipynb Cell 11'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(models, criterion, optimizers, dataloaders, num_epochs, epoch_loss)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_Random.ipynb#ch0000010vscode-remote?line=4'>5</a>\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(checkpoint_dir)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_Random.ipynb#ch0000010vscode-remote?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_Random.ipynb#ch0000010vscode-remote?line=7'>8</a>\u001b[0m     train_epoch(models, criterion, optimizers, dataloaders, num_epochs, epoch, epoch_loss)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_Random.ipynb#ch0000010vscode-remote?line=9'>10</a>\u001b[0m     \u001b[39m# Save a checkpoint\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_Random.ipynb#ch0000010vscode-remote?line=10'>11</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mand\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;32m/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_Random.ipynb Cell 10'\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(models, criterion, optimizers, dataloaders, num_epochs, epoch, epoch_loss)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_Random.ipynb#ch0000009vscode-remote?line=17'>18</a>\u001b[0m optimizers[\u001b[39m'\u001b[39m\u001b[39mbackbone\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_Random.ipynb#ch0000009vscode-remote?line=18'>19</a>\u001b[0m \u001b[39m# optimizers['module'].zero_grad()\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_Random.ipynb#ch0000009vscode-remote?line=20'>21</a>\u001b[0m _, _, target_loss \u001b[39m=\u001b[39m models[\u001b[39m'\u001b[39;49m\u001b[39mbackbone\u001b[39;49m\u001b[39m'\u001b[39;49m](inputs, labels, criterion)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_Random.ipynb#ch0000009vscode-remote?line=21'>22</a>\u001b[0m \u001b[39m# target_loss = target_loss.log_softmax(-1)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_Random.ipynb#ch0000009vscode-remote?line=22'>23</a>\u001b[0m \u001b[39m# target_loss = criterion(scores, labels)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_Random.ipynb#ch0000009vscode-remote?line=23'>24</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_Random.ipynb#ch0000009vscode-remote?line=34'>35</a>\u001b[0m \u001b[39m# m_module_loss   = LossPredLoss(pred_loss, target_loss, margin=MARGIN)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_Random.ipynb#ch0000009vscode-remote?line=35'>36</a>\u001b[0m \u001b[39m# loss            = m_backbone_loss + WEIGHT * m_module_loss\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Borange/home/jyji/devleop/TEMP/2022_bigdata/learning_loss/CRNN_with_Random.ipynb#ch0000009vscode-remote?line=37'>38</a>\u001b[0m pbar\u001b[39m.\u001b[39mset_description(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m) target_loss : \u001b[39m\u001b[39m{\u001b[39;00mtarget_loss\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/devleop/TEMP/2022_bigdata/learning_loss/models/crnn.py:59\u001b[0m, in \u001b[0;36mCRNN.forward\u001b[0;34m(self, X, y, criterion)\u001b[0m\n\u001b[1;32m     <a href='file:///home/jyji/devleop/TEMP/2022_bigdata/learning_loss/models/crnn.py?line=55'>56</a>\u001b[0m     target_lengths \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfull(size\u001b[39m=\u001b[39m(N,), fill_value\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint32)\n\u001b[1;32m     <a href='file:///home/jyji/devleop/TEMP/2022_bigdata/learning_loss/models/crnn.py?line=57'>58</a>\u001b[0m     \u001b[39m# out3 = out3.log_softmax(-1)\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/jyji/devleop/TEMP/2022_bigdata/learning_loss/models/crnn.py?line=58'>59</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(out3, y, input_lengths, target_lengths)\n\u001b[1;32m     <a href='file:///home/jyji/devleop/TEMP/2022_bigdata/learning_loss/models/crnn.py?line=60'>61</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m out3, [out, out2, out3], loss\n\u001b[1;32m     <a href='file:///home/jyji/devleop/TEMP/2022_bigdata/learning_loss/models/crnn.py?line=62'>63</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out3, [out, out2, out3], \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/loss.py:1743\u001b[0m, in \u001b[0;36mCTCLoss.forward\u001b[0;34m(self, log_probs, targets, input_lengths, target_lengths)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/loss.py?line=1741'>1742</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, log_probs: Tensor, targets: Tensor, input_lengths: Tensor, target_lengths: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/loss.py?line=1742'>1743</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mctc_loss(log_probs, targets, input_lengths, target_lengths, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblank, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/modules/loss.py?line=1743'>1744</a>\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mzero_infinity)\n",
      "File \u001b[0;32m~/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/functional.py:2599\u001b[0m, in \u001b[0;36mctc_loss\u001b[0;34m(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/functional.py?line=2591'>2592</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(log_probs, targets, input_lengths, target_lengths):\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/functional.py?line=2592'>2593</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/functional.py?line=2593'>2594</a>\u001b[0m         ctc_loss,\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/functional.py?line=2594'>2595</a>\u001b[0m         (log_probs, targets, input_lengths, target_lengths),\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/functional.py?line=2595'>2596</a>\u001b[0m         log_probs, targets, input_lengths, target_lengths,\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/functional.py?line=2596'>2597</a>\u001b[0m         blank\u001b[39m=\u001b[39mblank, reduction\u001b[39m=\u001b[39mreduction, zero_infinity\u001b[39m=\u001b[39mzero_infinity\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/functional.py?line=2597'>2598</a>\u001b[0m     )\n\u001b[0;32m-> <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/functional.py?line=2598'>2599</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mctc_loss(\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/functional.py?line=2599'>2600</a>\u001b[0m     log_probs, targets, input_lengths, target_lengths, blank, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), zero_infinity\n\u001b[1;32m   <a href='file:///home/jyji/miniconda3/envs/mytorch/lib/python3.8/site-packages/torch/nn/functional.py?line=2600'>2601</a>\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b4a27c98caeaa5630be10b8d406a4608184d11e4add7ee29d27ce8c7f4d0bc9a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('mytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
